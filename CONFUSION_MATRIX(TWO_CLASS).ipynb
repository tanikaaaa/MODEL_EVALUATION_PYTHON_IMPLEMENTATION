{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JDGBOtr7aU4",
        "outputId": "131ee90b-6dc1-49c4-a5a7-92518b7db1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix:\n",
            "[[3 1]\n",
            " [1 5]]\n",
            "\n",
            "Correct Positives: 5\n",
            "Incorrect Negatives: 1\n",
            "Incorrect Positives: 1\n",
            "Correct Negatives: 3\n",
            "\n",
            "Accuracy: 0.8\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 0.8333333333333334\n",
            "Specificity: 0.75\n",
            "F1 Score: 0.8333333333333334\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "actual_outcomes = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\n",
        "predicted_outcomes = np.array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1])\n",
        "\n",
        "pos_correct = pos_incorrect = neg_correct = neg_incorrect = 0\n",
        "\n",
        "for real, predicted in zip(actual_outcomes, predicted_outcomes):\n",
        "    if real == 1 and predicted == 1:\n",
        "        pos_correct += 1\n",
        "    elif real == 0 and predicted == 1:\n",
        "        neg_incorrect += 1\n",
        "    elif real == 1 and predicted == 0:\n",
        "        pos_incorrect += 1\n",
        "    elif real == 0 and predicted == 0:\n",
        "        neg_correct += 1\n",
        "\n",
        "total = pos_correct + neg_correct + neg_incorrect + pos_incorrect\n",
        "eval_accuracy = (pos_correct + neg_correct) / total\n",
        "eval_precision = pos_correct / (pos_correct + neg_incorrect) if (pos_correct + neg_incorrect) > 0 else 0\n",
        "eval_recall = pos_correct / (pos_correct + pos_incorrect) if (pos_correct + pos_incorrect) > 0 else 0\n",
        "eval_specificity = neg_correct / (neg_correct + neg_incorrect) if (neg_correct + neg_incorrect) > 0 else 0\n",
        "eval_f1 = 2 * (eval_precision * eval_recall) / (eval_precision + eval_recall) if (eval_precision + eval_recall) > 0 else 0\n",
        "\n",
        "print('Matrix:')\n",
        "print(confusion_matrix(actual_outcomes, predicted_outcomes))\n",
        "print()\n",
        "\n",
        "print('Correct Positives:', pos_correct)\n",
        "print('Incorrect Negatives:', pos_incorrect)\n",
        "print('Incorrect Positives:', neg_incorrect)\n",
        "print('Correct Negatives:', neg_correct)\n",
        "\n",
        "print()\n",
        "print(\"Accuracy:\", eval_accuracy)\n",
        "print(\"Precision:\", eval_precision)\n",
        "print(\"Recall:\", eval_recall)\n",
        "print(\"Specificity:\", eval_specificity)\n",
        "print(\"F1 Score:\", eval_f1)\n"
      ]
    }
  ]
}