{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLfGk9NY-uEw",
        "outputId": "65360100-d27e-49d7-c923-6cc8b205e79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[5 0 0]\n",
            " [1 3 1]\n",
            " [0 2 3]]\n",
            "\n",
            "Overall Accuracy: 0.7333333333333333\n",
            "\n",
            "Metrics for Each Class:\n",
            "\n",
            "Class 0:\n",
            "Precision: 0.8333333333333334\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9090909090909091\n",
            "Specificity: 0.9\n",
            "\n",
            "Class 1:\n",
            "Precision: 0.6\n",
            "Recall: 0.6\n",
            "F1 Score: 0.6\n",
            "Specificity: 0.8\n",
            "\n",
            "Class 2:\n",
            "Precision: 0.75\n",
            "Recall: 0.6\n",
            "F1 Score: 0.6666666666666665\n",
            "Specificity: 0.9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "actual_values = np.array([0, 1, 2, 0, 1, 2, 1, 0, 2, 1, 0, 2, 1, 2, 0])\n",
        "predicted_values = np.array([0, 1, 1, 0, 2, 2, 1, 0, 2, 0, 0, 2, 1, 1, 0])\n",
        "\n",
        "categories = np.unique(actual_values)\n",
        "num_classes = len(categories)\n",
        "matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
        "\n",
        "for true_label, predicted_label in zip(actual_values, predicted_values):\n",
        "    matrix[true_label][predicted_label] += 1\n",
        "\n",
        "metrics_per_class = {}\n",
        "\n",
        "for index, category in enumerate(categories):\n",
        "    correct_pred = matrix[index, index]\n",
        "    false_pos = np.sum(matrix[:, index]) - correct_pred\n",
        "    false_neg = np.sum(matrix[index, :]) - correct_pred\n",
        "    true_neg = np.sum(matrix) - (correct_pred + false_pos + false_neg)\n",
        "\n",
        "    precision_score = correct_pred / (correct_pred + false_pos) if (correct_pred + false_pos) > 0 else 0\n",
        "    recall_score = correct_pred / (correct_pred + false_neg) if (correct_pred + false_neg) > 0 else 0\n",
        "    f1_measure = 2 * (precision_score * recall_score) / (precision_score + recall_score) if (precision_score + recall_score) > 0 else 0\n",
        "    spec_score = true_neg / (true_neg + false_pos) if (true_neg + false_pos) > 0 else 0\n",
        "\n",
        "    metrics_per_class[category] = {\n",
        "        \"Precision\": precision_score,\n",
        "        \"Recall\": recall_score,\n",
        "        \"F1 Score\": f1_measure,\n",
        "        \"Specificity\": spec_score\n",
        "    }\n",
        "\n",
        "accuracy_overall = np.trace(matrix) / np.sum(matrix)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", matrix)\n",
        "print(\"\\nOverall Accuracy:\", accuracy_overall)\n",
        "print(\"\\nMetrics for Each Class:\")\n",
        "for cls, metric in metrics_per_class.items():\n",
        "    print(f\"\\nClass {cls}:\")\n",
        "    for key, val in metric.items():\n",
        "        print(f\"{key}: {val}\")\n"
      ]
    }
  ]
}