# -*- coding: utf-8 -*-
"""CONFUSION_MATRIX(TWO CLASS).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n1REc5xG1q53F_qhzwWIXwhcWEhLs-ma
"""

import numpy as np
from sklearn.metrics import confusion_matrix

actual_outcomes = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1])
predicted_outcomes = np.array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1])

pos_correct = pos_incorrect = neg_correct = neg_incorrect = 0

for real, predicted in zip(actual_outcomes, predicted_outcomes):
    if real == 1 and predicted == 1:
        pos_correct += 1
    elif real == 0 and predicted == 1:
        neg_incorrect += 1
    elif real == 1 and predicted == 0:
        pos_incorrect += 1
    elif real == 0 and predicted == 0:
        neg_correct += 1

total = pos_correct + neg_correct + neg_incorrect + pos_incorrect
eval_accuracy = (pos_correct + neg_correct) / total
eval_precision = pos_correct / (pos_correct + neg_incorrect) if (pos_correct + neg_incorrect) > 0 else 0
eval_recall = pos_correct / (pos_correct + pos_incorrect) if (pos_correct + pos_incorrect) > 0 else 0
eval_specificity = neg_correct / (neg_correct + neg_incorrect) if (neg_correct + neg_incorrect) > 0 else 0
eval_f1 = 2 * (eval_precision * eval_recall) / (eval_precision + eval_recall) if (eval_precision + eval_recall) > 0 else 0

print('Matrix:')
print(confusion_matrix(actual_outcomes, predicted_outcomes))
print()

print('Correct Positives:', pos_correct)
print('Incorrect Negatives:', pos_incorrect)
print('Incorrect Positives:', neg_incorrect)
print('Correct Negatives:', neg_correct)

print()
print("Accuracy:", eval_accuracy)
print("Precision:", eval_precision)
print("Recall:", eval_recall)
print("Specificity:", eval_specificity)
print("F1 Score:", eval_f1)